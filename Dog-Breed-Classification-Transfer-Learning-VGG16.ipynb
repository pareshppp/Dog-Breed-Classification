{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Breed Classification using PyTorch - Part 2\n",
    "## Transfer Learning with VGG16 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In the previous post: [Part-1](https://pareshppp.github.io/blogs/dog-breed-classification-scratch/), we had classified the images of dog breeds using a model that we created from scratch. Using that model we predicted the dog breeds with an accuracy of around 10%. With 133 dog breeds (target classes), random selection would have given us an accuracy of less than 1%. Compared to that our simple model performed reasonably well.\n",
    "\n",
    "But ~10% accuracy is still very low. We can use a more complex model for our problem but the more complex a model is, the more time and computing power it takes to train it. To get a high enough accuracy in our problem it would take days to train a sufficiently complex model on any personal computer.\n",
    "\n",
    "Instead, we are going to use a method called Transfer Learning to hasten the model training process.\n",
    "\n",
    "At a fundamental level, all images share the same basic features - Edges, Curves, Gradients, Patterns, etc. As such, we do not need to train the model to recognize these features every time. Since these features are stored in a model as weight parameters, we can re-use a pre-trained model to skip the time needed to train these weights. We only need to train the weights for the final classification layer based on our particular problem. This process is known as Transfer Learning.\n",
    "\n",
    "In this post we are going  to use a large but simple model called VGG-16.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Datasets\n",
    "\n",
    "The first step is to load-in the Images and check the total size of our dataset.\n",
    "\n",
    "> The Dog Images Dataset can be downloaded from here: [dog dataset](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip). Unzip the folder and place it in this project's home directory, at the location `/dogImages`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8351 total dog images.\n"
     ]
    }
   ],
   "source": [
    "# load filenames for dog images\n",
    "dog_files = np.array(glob(os.path.join('dogImages','*','*','*')))\n",
    "\n",
    "# print number of images in dataset\n",
    "print('There are %d total dog images.' % len(dog_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check CUDA Availability\n",
    "\n",
    "Check if GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU.\n"
     ]
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('Using GPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters\n",
    "\n",
    "Define the parameters needed in data loader and model creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_epochs = 5\n",
    "num_classes = 133\n",
    "num_workers = 0\n",
    "batch_size = 10\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders for the Dog Dataset\n",
    "\n",
    "In the next step we will do the following:\n",
    "1. Define Transformations that will be applied to the images using `torchvision.transforms`. Transformations are also known as Augmentation. This is a pre-processing step and it helps the model to generalize to new data much better.\n",
    "2. Load the image data using `torchvision.datasets.ImageFolder` and apply the transformations.\n",
    "3. Create Dataloaders using `torch.utils.data.DataLoader`.  \n",
    "\n",
    "> **Note:**\n",
    "- We have created dictionaries for all three steps that are divided into train, validation and test sets.\n",
    "- The Image Resize shape and mean & standard-deviation values for Normalization module were chosen so as to replicate the VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Specify data loaders\n",
    "trans = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_transfer = {\n",
    "    'train': datasets.ImageFolder(os.path.join('dogImages','train'), transform=trans['train']),\n",
    "    'valid': datasets.ImageFolder(os.path.join('dogImages','valid'), transform=trans['valid']),\n",
    "    'test': datasets.ImageFolder(os.path.join('dogImages','test'), transform=trans['test'])\n",
    "}\n",
    "\n",
    "loaders_transfer = {\n",
    "    'train': DataLoader(data_transfer['train'], batch_size=batch_size, num_workers=num_workers, shuffle=True),\n",
    "    'valid': DataLoader(data_transfer['valid'], batch_size=batch_size, num_workers=num_workers, shuffle=True),\n",
    "    'test': DataLoader(data_transfer['test'], batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Train DataLoader: 6680\n",
      "Size of Validation DataLoader: 835\n",
      "Size of Test DataLoader: 836\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of Train DataLoader: {len(loaders_transfer['train'].dataset)}\")\n",
    "print(f\"Size of Validation DataLoader: {len(loaders_transfer['valid'].dataset)}\")\n",
    "print(f\"Size of Test DataLoader: {len(loaders_transfer['test'].dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "Next, we will initialize the vgg16 **pre-trained** model using the `torchvision.models.vgg16` module. We will keep the whole model unchanged except the last classifier layer, where we change the number of output nodes to number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=133, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# specify model architecture \n",
    "model_transfer = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "# modify last layer of classifier\n",
    "model_transfer.classifier[6] = nn.Linear(4096, num_classes)\n",
    "\n",
    "print(model_transfer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze Feature Gradients\n",
    "We need to freeze the gradients for the feature part of the model as we do not want to re-train the weigths for those layers. We will only train the weights for the classifier section of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze gradients for model features\n",
    "for param in model_transfer.features.parameters():\n",
    "    param.require_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Loss Function and Optimizer\n",
    "\n",
    "We have chosen `CrossEntropyLoss` as our loss function and `Stochastic Gradient Descent` as our optimizer.\n",
    "\n",
    "> **Note:**\n",
    "Here we are only optimizing the weights for classifier part of the model. We will not change the weights for the features part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select loss function\n",
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "\n",
    "## select optimizer\n",
    "optimizer_transfer = optim.SGD(params=model_transfer.classifier.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate the Model\n",
    "\n",
    "We define a function for Training and Validation. It calculates a running train & validation loss and saves the model whenever the validation loss decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            if batch_idx % 200 == 0:\n",
    "                print(f\"Training Batch: {batch_idx}+/{len(loaders['train'])}\")\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            valid_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            if batch_idx % 200 == 0:\n",
    "                print(f\"Validation Batch: {batch_idx}+/{len(loaders['valid'])}\")\n",
    "\n",
    "        \n",
    "        train_loss = train_loss / len(loaders['train'].dataset)\n",
    "        valid_loss = valid_loss / len(loaders['valid'].dataset)\n",
    "        \n",
    "        # print training/validation statistics \n",
    "        print(f'Epoch: {epoch} \\tTraining Loss: {train_loss} \\tValidation Loss: {valid_loss}')\n",
    "        \n",
    "        # save the model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print(f'Validation loss decreased from {valid_loss_min} to {valid_loss}.\\nSaving Model...')\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 0+/668\n",
      "Training Batch: 200+/668\n",
      "Training Batch: 400+/668\n",
      "Training Batch: 600+/668\n",
      "Validation Batch: 0+/84\n",
      "Epoch: 1 \tTraining Loss: 2.233159229605498 \tValidation Loss: 1.1463432044326187\n",
      "Validation loss decreased from inf to 1.1463432044326187.\n",
      "Saving Model...\n",
      "Training Batch: 0+/668\n",
      "Training Batch: 200+/668\n",
      "Training Batch: 400+/668\n",
      "Training Batch: 600+/668\n",
      "Validation Batch: 0+/84\n",
      "Epoch: 2 \tTraining Loss: 1.570702178994874 \tValidation Loss: 0.9507174207243377\n",
      "Validation loss decreased from 1.1463432044326187 to 0.9507174207243377.\n",
      "Saving Model...\n",
      "Training Batch: 0+/668\n",
      "Training Batch: 200+/668\n",
      "Training Batch: 400+/668\n",
      "Training Batch: 600+/668\n",
      "Validation Batch: 0+/84\n",
      "Epoch: 3 \tTraining Loss: 1.4183635966863462 \tValidation Loss: 0.9120735898167788\n",
      "Validation loss decreased from 0.9507174207243377 to 0.9120735898167788.\n",
      "Saving Model...\n",
      "Training Batch: 0+/668\n",
      "Training Batch: 200+/668\n",
      "Training Batch: 400+/668\n",
      "Training Batch: 600+/668\n",
      "Validation Batch: 0+/84\n",
      "Epoch: 4 \tTraining Loss: 1.3522749468014983 \tValidation Loss: 0.91904990312582\n",
      "Training Batch: 0+/668\n",
      "Training Batch: 200+/668\n",
      "Training Batch: 400+/668\n",
      "Training Batch: 600+/668\n",
      "Validation Batch: 0+/84\n",
      "Epoch: 5 \tTraining Loss: 1.3099311252910935 \tValidation Loss: 0.7952524953170451\n",
      "Validation loss decreased from 0.9120735898167788 to 0.7952524953170451.\n",
      "Saving Model...\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "if use_cuda:\n",
    "    model_transfer = model_transfer.cuda()\n",
    "\n",
    "model_transfer = train(n_epochs, loaders_transfer, model_transfer, \\\n",
    "                       optimizer_transfer, criterion_transfer, use_cuda, 'model_transfer.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model that got the best validation accuracy (uncomment the line below)\n",
    "model_transfer.load_state_dict(torch.load('model_transfer.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "We compare the predicted outputs with target to get the number of correct predictions and then calculate the pecentage accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.928593\n",
      "\n",
      "\n",
      "Test Accuracy: 73% (612/836)\n"
     ]
    }
   ],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss += loss.item() * data.size(0)\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    test_loss = test_loss / len(loaders['test'].dataset)\n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# call test function \n",
    "test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "With only 5 epochs of training we achieved an accuracy of over 70%. The loss was still decreasing, so we may have been able to get even better performance with more training. This is a huge improvement over the ~10% accuracy we got using the model we created from scratch in [Part-1](https://pareshppp.github.io/blogs/dog-breed-classification-scratch/).\n",
    "\n",
    "VGG16 is not the most advanced model architecture for image recognition. We can get near human level accuracy by using other model architectures such as ResNet. We will look into that in a future post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
